1. Use the below given data set
DataSet:https://www.kaggle.com/hugodarwood/epirecipes/data
2. Perform the below given activities:
a. apply K-means clustering to identify similar recipies
b. apply K-means clustering to identify similar attributes
c. how many unique recipies that people order often
d. what are their typical profiles

setwd("D:/R")
library(readr)
epi_r <- read.csv("epi_r.csv")
View(epi_r)
df<-epi_r
df[df==""] <- NA
df1<-na.exclude(df)
View(df1)
str(df1)
library(factoextra)
library("factoextra")
df <- df1[1:1000, 1:6]
na.exclude(df)
View(df)
head(df[, 1:6])
# Prepare Data
df <- na.omit(df) # listwise deletion of missing
#df <- scale(df) # standardize variables
View(df)
set.seed(1234)
ind = sample(1:nrow(df),0.8*nrow(df),replace = F)
df_train =df[ind,-1]
df_test = df[-ind,-1]
summary(df)
dim(df)
# outlier definition
# x > Q3+1.5*IQR - positive side outlier
# x < Q1-1.5*IQR - negative or lower side outlier
par(mfrow=c(2,3))
(boxplot(df1$rating)$out);(boxplot(df1$calories)$out);(boxplot(df1$protein)$out);(boxplot(df1$fat)$out);(boxplot(df1$sodium)$out)
apply(df,2,range)
apply(df,2,summary)
# KMeans - comes from Rcmdr library
# Kmeans- from amap library
# kmeans- from stats library
# steps in k-means clustering
#1- preprocessing the data (impute missing values, remove outliers, feature trasnformation)
#2- scaling or standardization of data set
#3- decide the number of clusters (value of K)
#4- iterate over the samples to create clusters
#5- decide the distance measure
#6- calculate the group accuracy
# scaling of data
df_train1 <- scale(df_train)
head(df_train1)
class(df_train1)
# screeplot approach to decide the number of clusters
km = kmeans(df_train1,1)
km$withinss
km$tot.withinss
km = kmeans(df_train1,2)
km$withinss
km$tot.withinss
km = kmeans(df_train1,3)
km$withinss
km$tot.withinss
km = kmeans(df_train1,4)
km$withinss
km$tot.withinss
km = kmeans(df_train1,5)
km$withinss
km$tot.withinss
km = kmeans(df_train1,6)
km$withinss
km$tot.withinss
km = kmeans(df_train1,7)
km$withinss
km$tot.withinss
km = kmeans(df_train1,8)
km$withinss
km$tot.withinss
km = kmeans(df_train1,9)
km$withinss
km$tot.withinss
km = kmeans(df_train1,10)
km$withinss
km$tot.withinss
dev.off()
sumsq=NULL
for (i in 1:25)
sumsq[i] = sum(kmeans(df_train,centers=i, iter.max = 1000, nstart=i, algorithm='Forgy')$withinss)
plot(1:25,sumsq,type='b', main='Screeplot showing within group sum of squares')
km = kmeans(df_train1,3)
km$withinss
km$tot.withinss
class(km$cluster)
summary(km)
km$centers
as.numeric(km$cluster)
length(km$cluster)
dim(df_train)
class(df_train)
df_train$cl <- km$cluster
head(df_train)
# profiles of clusters
aggregate(df_train[,1:5],list(df_train[,6]),mean)
table(df1$rating)
table(df1$calories)
table(df1$X22.minute.meals)
table(df1$sodium)
library(cluster)
clusplot(df_train,df_train$cl,cex=0.9,color=T,shade=T, labels=4,lines=0)
#HC clustering or Hierarchical Clustering
# distance (euclidean, manhattan, cosine distance)
# Divisive method (top down)
# Agglomorative method (bottom up)
df_train = df_train[,-5]
head(df_train)
str(df_train)
# compute the distance metrix
d1 <- dist(df_train,method='euclidean')
summary(d1)
# HC
fit <- hclust(d1,method = 'ward.D2')
plot(fit)
# single, double, average, ward, ward.D2
# agglomorative method
fit <- agnes(d1,metric='euclidean',method = 'ward')
plot(fit)
# divisive method
fit <- diana(d1,metric='euclidean')
plot(fit)

setwd("D:/R") 
library(readr) 
epi_r <- read.csv("epi_r.csv")

View(epi_r) 
df<-epi_r 
df[df==""] <- NA 
df1<-na.exclude(df) 
View(df1) 
str(df1)
library(factoextra)

df <- df1[1:1000, 1:6] 
na.exclude(df)
View(df) 
head(df[, 1:6])

# Prepare Data 
df <- na.omit(df) 
# listwise deletion of missing 
#df <- scale(df) 
# standardize variables 
View(df) 
set.seed(1234) 
ind = sample(1:nrow(df),0.8*nrow(df),replace = F) 
df_train =df[ind,-1] 
df_test = df[-ind,-1] 
summary(df)

dim(df)

# outlier definition 
# x > Q3+1.5*IQR - positive side outlier 
# x < Q1-1.5*IQR - negative or lower side outlier

par(mfrow=c(2,3)) 
(boxplot(df1$rating)$out);(boxplot(df1$calories)$out);(boxplot(df1$protein)$out);(boxplot(df1$fat)$out);(boxplot(df1$sodium)$out)

apply(df,2,range)

apply(df,2,summary)

# scaling of data 
df_train1 <- scale(df_train) 
head(df_train1)

class(df_train1)

# screeplot approach to decide the number of clusters 
km = kmeans(df_train1,1) 
km$withinss
km$tot.withinss

km = kmeans(df_train1,2) 
km$withinss

km$tot.withinss

km = kmeans(df_train1,3) 
km$withinss
km$tot.withinss
km = kmeans(df_train1,4) 
km$withinss
km$tot.withinss
km = kmeans(df_train1,5) 
km$withinss

km$tot.withinss

km = kmeans(df_train1,6) 
km$withinss

km$tot.withinss

km = kmeans(df_train1,7) 
km$withinss
km$tot.withinss
km = kmeans(df_train1,8) 
km$withinss
km$tot.withinss
km = kmeans(df_train1,9) 
km$withinss

km$tot.withinss

km = kmeans(df_train1,10) 
km$withinss
km$tot.withinss

dev.off()

sumsq=NULL 
for (i in 1:25) 
sumsq[i] = sum(kmeans(df_train,centers=i, iter.max = 1000, nstart=i, algorithm='Forgy')$withinss)

plot(1:25,sumsq,type='b', main='Screeplot showing within group sum of squares')

km = kmeans(df_train1,3) 
km$withinss

km$tot.withinss
class(km$cluster)
summary(km)

km$centers

as.numeric(km$cluster)

length(km$cluster) 
dim(df_train) 
class(df_train) 
df_train$cl <- km$cluster 
head(df_train)
table(df1$rating)

table(df1$calories)

table(df1$X22.minute.meals)
table(df1$sodium)
library(cluster) 
clusplot(df_train,df_train$cl,cex=0.9,color=T,shade=T, labels=4,lines=0)
df_train = df_train[,-5] 
head(df_train)

# compute the distance metrix 
d1 <- dist(df_train,method='euclidean') 
summary(d1)

# HC 
fit <- hclust(d1,method = 'ward.D2') 
plot(fit) 

# agglomorative method 
fit <- agnes(d1,metric='euclidean',method = 'ward') 
plot(fit) 
# divisive method 
fit <- diana(d1,metric='euclidean') 
plot(fit)




Based on the assumption that o and 1 are indication of people order recipes, we have modified the data base , based on higher no of 1 and sorted then named it as epir_1 and the cluster analysis is performed.
Based on the analysis the aggregate group and cluster are given below.

head(df_train)
# profiles of clusters 
aggregate(df_train[,1:5],list(df_train[,6]),mean)

setwd("D:/R") 
library(readr) 
epi_r1 <- read.csv("epi_r1.csv") 
View(epi_r1) 
df<-epi_r1 df[df==""] <- NA 
df1<-na.exclude(df) 
View(df1) 
str(df1)


library(factoextra)

df <- df1[1:1000, 1:6] 
na.exclude(df)

View(df) 
head(df[, 1:6])

# Prepare Data 
df <- na.omit(df) # listwise deletion of missing 
View(df) 
set.seed(1234) 
ind = sample(1:nrow(df),0.8*nrow(df),replace = F) 
df_train =df[ind,-1] 
df_test = df[-ind,-1]

summary(df)
dim(df)

# outlier definition 
# x > Q3+1.5*IQR - positive side outlier 
# x < Q1-1.5*IQR - negative or lower side outlier 

par(mfrow=c(2,3)) 
(boxplot(df1$rating)$out);(boxplot(df1$calories)$out);(boxplot(df1$protein)$out);(boxplot(df1$fat)$out);(boxplot(df1$sodium)$out)
apply(df,2,range)
apply(df,2,summary)

# scaling of data 
df_train1 <- scale(df_train) 
head(df_train1)
class(df_train1)
++++++++++++++++++++++++++++++++++++++++++++++++

km = kmeans(df_train1,1) 
km$withinss 
km$tot.withinss 
km = kmeans(df_train1,2) 
km$withinss 
km$tot.withinss 
km = kmeans(df_train1,3) 
km$withinss 
km$tot.withinss 
km = kmeans(df_train1,4) 
km$withinss 
km$tot.withinss 
km = kmeans(df_train1,5) 
km$withinss 
km$tot.withinss 
km = kmeans(df_train1,6) 
km$withinss

km$tot.withinss 
km = kmeans(df_train1,7) 
km$withinss 
km$tot.withinss 
km = kmeans(df_train1,8) 
km$withinss 
km$tot.withinss 
km = kmeans(df_train1,9) 
km$withinss 
km$tot.withinss 
km = kmeans(df_train1,10) 
km$withinss 
km$tot.withinss 
dev.off() 
sumsq=NULL 
for (i in 1:25) 
sumsq[i] = sum(kmeans(df_train,centers=i, iter.max = 1000, nstart=i,
algorithm='Forgy')$withinss) 
plot(1:25,sumsq,type='b', main='Screeplot showing within group sum of squares') 
km = kmeans(df_train1,3) 
km$withinss 
km$tot.withinss 
class(km$cluster) 
summary(km)
km$centers
as.numeric(km$cluster)

length(km$cluster)
dim(df_train)

class(df_train) 
df_train$cl <- km$cluster 
head(df_train)

# profiles of clusters 
aggregate(df_train[,1:5],list(df_train[,6]),mean)
table(df1$rating)
table(df1$calories)
table(df1$X22.minute.meals)
table(df1$sodium)
library(cluster) 
clusplot(df_train,df_train$cl,cex=0.9,color=T,shade=T, labels=4,lines=0)
# Agglomorative method (bottom up) 
df_train = df_train[,-5] 
head(df_train)
str(df_train)

# compute the distance metrix 
d1 <- dist(df_train,method='euclidean') 
summary(d1)

# HC 
fit <- hclust(d1,method = 'ward.D2') 
plot(fit)

